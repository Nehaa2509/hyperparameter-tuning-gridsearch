# Hyperparameter Tuning using GridSearchCV  
## AI & ML Internship â€“ Task 16

## ğŸ“Œ Objective
The objective of this task is to optimize a machine learning model using GridSearchCV and compare its performance with a default model.

---

## ğŸ“Š Dataset
Breast Cancer Dataset (from sklearn library)

- Binary classification problem  
- Classes: Malignant (0) and Benign (1)  
- 569 samples  
- 30 numerical features  

---

## ğŸ›  Tools Used
- Python  
- Pandas  
- Scikit-learn  
- Jupyter Notebook / Google Colab  

---

## ğŸ” Workflow
1. Loaded dataset from sklearn  
2. Split data into training and testing sets (80/20 with stratification)  
3. Trained baseline Random Forest model  
4. Defined hyperparameter grid  
5. Applied GridSearchCV with 5-fold cross-validation  
6. Extracted best parameters  
7. Evaluated tuned model  
8. Compared performance  

---

## âš™ Hyperparameter Grid
```
n_estimators: [50, 100, 200]
max_depth: [None, 10, 20]
min_samples_split: [2, 5, 10]
```

Total combinations tested: 27  
Total models trained with 5-fold CV: 135  

---

## ğŸ“ˆ Results
Default Model Accuracy: XX.XX  

Best Parameters:
```
{
 'n_estimators': ...,
 'max_depth': ...,
 'min_samples_split': ...
}
```

Tuned Model Accuracy: XX.XX  

---

## ğŸ§  Concepts Covered
- Hyperparameters  
- GridSearchCV  
- Cross-validation  
- Model optimization  
- Avoiding data leakage  
- GridSearch vs RandomSearch  

---

## ğŸ“ Repository Structure
```
hyperparameter-tuning-gridsearch/
â”‚
â”œâ”€â”€ task16_gridsearch.ipynb
â””â”€â”€ README.md
```

---

## âœ… Conclusion
This project demonstrates how hyperparameter tuning improves model performance and ensures better generalization using cross-validation.
